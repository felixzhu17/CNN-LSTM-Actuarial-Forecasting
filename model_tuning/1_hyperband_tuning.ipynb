{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhufe\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\zhufe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\zhufe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "c:\\Users\\zhufe\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "from keras_tuner.tuners import Hyperband\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, LSTM, Flatten, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from methods.nn import get_model_name\n",
    "from methods.data_methods import prepare_model_data\n",
    "from methods.clean_data import Data_Prep\n",
    "from methods.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_no_CNN(hp):\n",
    "    model = Sequential()\n",
    "    Learning_rate = hp.Choice(\"learning_rate\", values=[1e-3, 1e-5])\n",
    "    LSTM_layers = hp.Int(\"LSTM_layers\", 0, 2, step=1)\n",
    "    Dense_layers = hp.Int(\"Dense_layers\", 0, 2, step=1)\n",
    "    Dropout_prob = hp.Float(\"Dropout_prob\", 0.2, 0.5, step=0.3)\n",
    "\n",
    "    if LSTM_layers == 0:\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                hp.Int(\"input_LSTM\", min_value=32, max_value=192, step=32),\n",
    "                batch_input_shape=(\n",
    "                    BATCH_SIZE,\n",
    "                    data[\"train_X\"].shape[1],\n",
    "                    data[\"train_X\"].shape[2],\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                hp.Int(\"input_LSTM\", min_value=32, max_value=192, step=32),\n",
    "                batch_input_shape=(\n",
    "                    BATCH_SIZE,\n",
    "                    data[\"train_X\"].shape[1],\n",
    "                    data[\"train_X\"].shape[2],\n",
    "                ),\n",
    "                return_sequences=True,\n",
    "            )\n",
    "        )\n",
    "    model.add(Dropout(Dropout_prob))\n",
    "\n",
    "    for i in range(LSTM_layers):\n",
    "        if i == LSTM_layers - 1:\n",
    "            model.add(\n",
    "                LSTM(hp.Int(f\"LSTM_{i}_units\", min_value=32, max_value=192, step=32))\n",
    "            )\n",
    "        else:\n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    hp.Int(f\"LSTM_{i}_units\", min_value=32, max_value=192, step=32),\n",
    "                    return_sequences=True,\n",
    "                )\n",
    "            )\n",
    "        model.add(Dropout(Dropout_prob))\n",
    "\n",
    "    for i in range(Dense_layers):\n",
    "        model.add(\n",
    "            Dense(hp.Int(f\"Dense_{i}_units\", min_value=32, max_value=192, step=32))\n",
    "        )\n",
    "        model.add(Dropout(Dropout_prob))\n",
    "\n",
    "    model.add(Dense(len(dataset[\"Y_variables\"]), activation=\"linear\"))\n",
    "    opt = Adam(learning_rate=Learning_rate, decay=1e-6)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    Learning_rate = hp.Choice(\"learning_rate\", values=[1e-3, 1e-5])\n",
    "    CNN_layers = hp.Int(\"CNN_layers\", 0, 2, step=1)\n",
    "    LSTM_layers = hp.Int(\"LSTM_layers\", 0, 2, step=1)\n",
    "    Dense_layers = hp.Int(\"Dense_layers\", 0, 2, step=1)\n",
    "    Dropout_prob = hp.Float(\"Dropout_prob\", 0.2, 0.5, step=0.3)\n",
    "\n",
    "    for i in range(CNN_layers):\n",
    "        if i == 0:\n",
    "            model.add(\n",
    "                Conv1D(\n",
    "                    filters=hp.Int(\n",
    "                        f\"CNN_{i}_filters\", min_value=32, max_value=128, step=32\n",
    "                    ),\n",
    "                    kernel_size=2,\n",
    "                    batch_input_shape=(\n",
    "                        BATCH_SIZE,\n",
    "                        data[\"train_X\"].shape[1],\n",
    "                        data[\"train_X\"].shape[2],\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            model.add(\n",
    "                Conv1D(\n",
    "                    filters=hp.Int(\n",
    "                        f\"CNN_{i}_filters\", min_value=32, max_value=128, step=32\n",
    "                    ),\n",
    "                    kernel_size=2,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Dropout(Dropout_prob))\n",
    "\n",
    "    for i in range(LSTM_layers):\n",
    "        if i == LSTM_layers - 1:\n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    hp.Int(f\"LSTM_{i}_units\", min_value=32, max_value=192, step=32),\n",
    "                    batch_input_shape=(\n",
    "                        BATCH_SIZE,\n",
    "                        data[\"train_X\"].shape[1],\n",
    "                        data[\"train_X\"].shape[2],\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    hp.Int(f\"LSTM_{i}_units\", min_value=32, max_value=192, step=32),\n",
    "                    batch_input_shape=(\n",
    "                        BATCH_SIZE,\n",
    "                        data[\"train_X\"].shape[1],\n",
    "                        data[\"train_X\"].shape[2],\n",
    "                    ),\n",
    "                    return_sequences=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        model.add(Dropout(Dropout_prob))\n",
    "\n",
    "    if LSTM_layers == 0:\n",
    "        model.add(Flatten())\n",
    "\n",
    "    for i in range(Dense_layers):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                hp.Int(f\"Dense_{i}_units\", min_value=32, max_value=192, step=32),\n",
    "                batch_input_shape=(\n",
    "                    BATCH_SIZE,\n",
    "                    data[\"train_X\"].shape[1],\n",
    "                    data[\"train_X\"].shape[2],\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        model.add(Dropout(Dropout_prob))\n",
    "\n",
    "    model.add(Dense(len(dataset[\"Y_variables\"]), activation=\"linear\"))\n",
    "\n",
    "    opt = Adam(learning_rate=Learning_rate, decay=1e-6)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 06s]\n",
      "val_loss: 8.850561789586209e-05\n",
      "\n",
      "Best val_loss So Far: 8.850561789586209e-05\n",
      "Total elapsed time: 00h 00m 14s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for end_year in PERIODS_MAP.values():\n",
    "    for look_back_year in LOOK_BACK_YEARS:\n",
    "        for variable in TARGET_VARIABLES:\n",
    "            for number_of_pca in NUMBER_OF_PCAS:\n",
    "                for output_steps in OUTPUT_STEPS:\n",
    "\n",
    "                    look_back_steps = int(look_back_year * 12)\n",
    "                    data_prep = Data_Prep(DATA_PATH, TRANSFORM_PATH)\n",
    "                    data_prep.transform_to_supervised_learning(\n",
    "                        NA_CUTOFF,\n",
    "                        [variable],\n",
    "                        output_steps,\n",
    "                        start=f\"{START_YEAR}-01-01\",\n",
    "                        end=f\"{end_year}-01-01\",\n",
    "                    )\n",
    "                    dataset = data_prep.supervised_dataset\n",
    "                    full_dataset = dataset[\"transformed_data\"]\n",
    "\n",
    "                    NAME = get_model_name(\n",
    "                        end_year,\n",
    "                        variable,\n",
    "                        FREQUENCY,\n",
    "                        output_steps,\n",
    "                        look_back_year,\n",
    "                        REMOVE_OUTLIER,\n",
    "                        VAL_YEARS,\n",
    "                        number_of_pca,\n",
    "                    )\n",
    "\n",
    "                    x = os.listdir(TUNING_PARAMS_PATH)\n",
    "                    if f\"{NAME}.pkl\" in x:\n",
    "                        print(f\"{NAME} exists\")\n",
    "\n",
    "                    else:\n",
    "                        print(f\"Training {NAME}\")\n",
    "                        data = prepare_model_data(\n",
    "                            window=full_dataset,\n",
    "                            X_variables=dataset[\"X_variables\"],\n",
    "                            Y_variables=dataset[\"Y_variables\"],\n",
    "                            val_steps=VAL_STEPS,\n",
    "                            look_back=look_back_steps,\n",
    "                            test_steps=TEST_STEPS,\n",
    "                            remove_outlier=REMOVE_OUTLIER,\n",
    "                            number_of_pca=number_of_pca,\n",
    "                            target_variables=TARGET_VARIABLES,\n",
    "                        )\n",
    "\n",
    "                        # Adjust size to match batch\n",
    "                        data[\"train_X\"] = data[\"train_X\"][\n",
    "                            len(data[\"train_X\"]) % BATCH_SIZE :\n",
    "                        ]\n",
    "                        data[\"train_Y\"] = data[\"train_Y\"][\n",
    "                            len(data[\"train_Y\"]) % BATCH_SIZE :\n",
    "                        ]\n",
    "\n",
    "                        if look_back_year < 1:\n",
    "                            NN_func = build_model_no_CNN\n",
    "                        else:\n",
    "                            NN_func = build_model\n",
    "\n",
    "                        tuner = Hyperband(\n",
    "                            NN_func,\n",
    "                            objective=\"val_loss\",\n",
    "                            max_epochs=15,\n",
    "                            executions_per_trial=2,\n",
    "                            hyperband_iterations=1,\n",
    "                            directory=TUNING_TUNER_PATH,\n",
    "                            overwrite=False,\n",
    "                            project_name=f\"{NAME}\",\n",
    "                        )\n",
    "\n",
    "                        tuner.search(\n",
    "                            x=data[\"train_X\"],\n",
    "                            y=data[\"train_Y\"],\n",
    "                            verbose=2,\n",
    "                            epochs=15,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            callbacks=[\n",
    "                                tf.keras.callbacks.EarlyStopping(\"val_loss\", patience=3)\n",
    "                            ],\n",
    "                            validation_data=(data[\"val_X\"], data[\"val_Y\"]),\n",
    "                        )\n",
    "\n",
    "                        tuner.save()\n",
    "                        best_hp = tuner.get_best_hyperparameters()[0]\n",
    "                        best_model = tuner.hypermodel.build(best_hp)\n",
    "\n",
    "                        with open(os.path.join(TUNING_PARAMS_PATH, f\"{NAME}.json\"), 'w') as f:\n",
    "                            json.dump(best_hp.values, f)\n",
    "\n",
    "                        best_model.save(os.path.join(TUNING_MODELS_PATH, f\"{NAME}.h5\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('test_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "035b554e3549a2b2468cb30fcc59f94af0d4b55bc9ff889feec7e35d12f7b752"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
